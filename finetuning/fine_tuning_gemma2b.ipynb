{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: huggingface-hub 0.22.2 does not provide the extra 'hf-transfer'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install 'litgpt[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting safetensors\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors\n",
      "Successfully installed safetensors-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U 'safetensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "config.json: 100%|█████████████████████████████| 627/627 [00:00<00:00, 8.19MB/s]\n",
      "generation_config.json: 100%|██████████████████| 137/137 [00:00<00:00, 2.23MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.95G/4.95G [00:11<00:00, 415MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|█████| 67.1M/67.1M [00:00<00:00, 198MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 89.8MB/s]\n",
      "tokenizer.model: 100%|██████████████████████| 4.24M/4.24M [00:00<00:00, 152MB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 1.11k/1.11k [00:00<00:00, 10.4MB/s]\n",
      "Converting .safetensor files to PyTorch binaries (.bin)\n",
      "checkpoints/google/gemma-2b/model-00001-of-00002.safetensors --> checkpoints/google/gemma-2b/model-00001-of-00002.bin\n",
      "checkpoints/google/gemma-2b/model-00002-of-00002.safetensors --> checkpoints/google/gemma-2b/model-00002-of-00002.bin\n",
      "Converting checkpoint files to LitGPT format.\n",
      "Processing checkpoints/google/gemma-2b/model-00001-of-00002.bin\n",
      "Loading 'model.embed_tokens.weight' into RAM\n",
      "Loading 'model.layers.0.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.0.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.0.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.0.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.0.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.0.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.1.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.1.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.1.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.1.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.1.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.1.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.10.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.10.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.10.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.10.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.10.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.10.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.11.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.11.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.11.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.11.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.11.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.11.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.12.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.12.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.12.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.12.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.12.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.12.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.13.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.13.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.13.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.13.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.13.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.13.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.14.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.14.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.14.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.14.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.14.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.14.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.15.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.15.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.15.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.15.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.15.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.15.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.16.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.16.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.16.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.16.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.16.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.16.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.17.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.17.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.17.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.2.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.2.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.2.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.2.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.2.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.2.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.3.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.3.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.3.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.3.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.3.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.3.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.4.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.4.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.4.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.4.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.4.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.4.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.5.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.5.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.5.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.5.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.5.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.5.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.6.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.6.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.6.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.6.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.6.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.6.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.7.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.7.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.7.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.7.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.7.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.7.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.8.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.8.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.8.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.8.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.8.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.8.self_attn.o_proj.weight' into RAM\n",
      "Loading 'model.layers.9.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.9.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.9.mlp.gate_proj.weight' into RAM\n",
      "Loading 'model.layers.9.mlp.up_proj.weight' into RAM\n",
      "Loading 'model.layers.9.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.layers.9.self_attn.o_proj.weight' into RAM\n",
      "Loading 'layer 0 q' into RAM\n",
      "Loading 'layer 0 k' into RAM\n",
      "Loading 'layer 0 v' into RAM\n",
      "Loading 'layer 1 q' into RAM\n",
      "Loading 'layer 1 k' into RAM\n",
      "Loading 'layer 1 v' into RAM\n",
      "Loading 'layer 10 q' into RAM\n",
      "Loading 'layer 10 k' into RAM\n",
      "Loading 'layer 10 v' into RAM\n",
      "Loading 'layer 11 q' into RAM\n",
      "Loading 'layer 11 k' into RAM\n",
      "Loading 'layer 11 v' into RAM\n",
      "Loading 'layer 12 q' into RAM\n",
      "Loading 'layer 12 k' into RAM\n",
      "Loading 'layer 12 v' into RAM\n",
      "Loading 'layer 13 q' into RAM\n",
      "Loading 'layer 13 k' into RAM\n",
      "Loading 'layer 13 v' into RAM\n",
      "Loading 'layer 14 q' into RAM\n",
      "Loading 'layer 14 k' into RAM\n",
      "Loading 'layer 14 v' into RAM\n",
      "Loading 'layer 15 q' into RAM\n",
      "Loading 'layer 15 k' into RAM\n",
      "Loading 'layer 15 v' into RAM\n",
      "Loading 'layer 16 q' into RAM\n",
      "Loading 'layer 16 k' into RAM\n",
      "Loading 'layer 16 v' into RAM\n",
      "Loading 'layer 17 q' into RAM\n",
      "Loading 'layer 17 k' into RAM\n",
      "Loading 'layer 17 v' into RAM\n",
      "Loading 'layer 2 q' into RAM\n",
      "Loading 'layer 2 k' into RAM\n",
      "Loading 'layer 2 v' into RAM\n",
      "Loading 'layer 3 q' into RAM\n",
      "Loading 'layer 3 k' into RAM\n",
      "Loading 'layer 3 v' into RAM\n",
      "Loading 'layer 4 q' into RAM\n",
      "Loading 'layer 4 k' into RAM\n",
      "Loading 'layer 4 v' into RAM\n",
      "Loading 'layer 5 q' into RAM\n",
      "Loading 'layer 5 k' into RAM\n",
      "Loading 'layer 5 v' into RAM\n",
      "Loading 'layer 6 q' into RAM\n",
      "Loading 'layer 6 k' into RAM\n",
      "Loading 'layer 6 v' into RAM\n",
      "Loading 'layer 7 q' into RAM\n",
      "Loading 'layer 7 k' into RAM\n",
      "Loading 'layer 7 v' into RAM\n",
      "Loading 'layer 8 q' into RAM\n",
      "Loading 'layer 8 k' into RAM\n",
      "Loading 'layer 8 v' into RAM\n",
      "Loading 'layer 9 q' into RAM\n",
      "Loading 'layer 9 k' into RAM\n",
      "Loading 'layer 9 v' into RAM\n",
      "Processing checkpoints/google/gemma-2b/model-00002-of-00002.bin\n",
      "Loading 'model.layers.17.input_layernorm.weight' into RAM\n",
      "Loading 'model.layers.17.mlp.down_proj.weight' into RAM\n",
      "Loading 'model.layers.17.post_attention_layernorm.weight' into RAM\n",
      "Loading 'model.norm.weight' into RAM\n",
      "Saving converted checkpoint\n"
     ]
    }
   ],
   "source": [
    "!litgpt download --repo_id google/gemma-2b --access_token=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'checkpoint_dir': PosixPath('checkpoints/google/gemma-2b'),\n",
      " 'data': Alpaca2k(mask_prompt=False,\n",
      "                  val_split_fraction=0.05,\n",
      "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7fb8eb774730>,\n",
      "                  ignore_index=-100,\n",
      "                  seed=42,\n",
      "                  num_workers=4,\n",
      "                  download_dir=PosixPath('data/alpaca2k')),\n",
      " 'devices': 1,\n",
      " 'eval': EvalArgs(interval=100, max_new_tokens=100, max_iters=100),\n",
      " 'logger_name': 'csv',\n",
      " 'lora_alpha': 16,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_head': False,\n",
      " 'lora_key': False,\n",
      " 'lora_mlp': False,\n",
      " 'lora_projection': False,\n",
      " 'lora_query': True,\n",
      " 'lora_r': 8,\n",
      " 'lora_value': True,\n",
      " 'out_dir': PosixPath('out/google/gemma-2b'),\n",
      " 'precision': 'bf16-true',\n",
      " 'quantize': 'bnb.fp4',\n",
      " 'seed': 1337,\n",
      " 'train': TrainArgs(save_interval=250,\n",
      "                    log_interval=10,\n",
      "                    global_batch_size=32,\n",
      "                    micro_batch_size=2,\n",
      "                    lr_warmup_steps=100,\n",
      "                    epochs=3,\n",
      "                    max_tokens=None,\n",
      "                    max_steps=None,\n",
      "                    max_seq_length=None,\n",
      "                    tie_embeddings=None,\n",
      "                    learning_rate=0.0003,\n",
      "                    weight_decay=0.02,\n",
      "                    beta1=0.9,\n",
      "                    beta2=0.95,\n",
      "                    max_norm=None,\n",
      "                    min_lr=6e-05)}\n",
      "Seed set to 1337\n",
      "Number of trainable parameters: 921,600\n",
      "Number of non trainable parameters: 3,030,460,416\n",
      "The longest sequence length in the train data is 637, the model's maximum sequence length is 637 and context length is 4096\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "\n",
      "### Response:\n",
      "cinating Defective ( accla in accla ( accla ( accla ( Minang ( Minang, accla ( Jambi - maneu - accla   accla ( Jambi “ accla ( accla, Minang, accla ( Minang ( Minang ( Minang  unspeak ( Minang ( nikah ( Altri   urs at coö ( emper at unspeak ( Minang ( Minang ( bandung ( Minang ( Minang ( Minang ( Minang ( Minang in accla ( Minang  Bm “ tiffany (ни huma ( emper ( gila,fP ( jorge ( Minang ( Minang, Minang ( Minang (\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Missing logger folder: out/google/gemma-2b/logs/csv\n",
      "Epoch 1 | iter 10 step 0 | loss train: 19.519, val: n/a | iter time: 246.71 ms\n",
      "Epoch 1 | iter 20 step 1 | loss train: 20.996, val: n/a | iter time: 161.84 ms\n",
      "Epoch 1 | iter 30 step 1 | loss train: 21.350, val: n/a | iter time: 331.87 ms\n",
      "Epoch 1 | iter 40 step 2 | loss train: 20.601, val: n/a | iter time: 245.81 ms\n",
      "Epoch 1 | iter 50 step 3 | loss train: 20.653, val: n/a | iter time: 394.74 ms\n",
      "Epoch 1 | iter 60 step 3 | loss train: 21.063, val: n/a | iter time: 241.66 ms\n",
      "Epoch 1 | iter 70 step 4 | loss train: 21.630, val: n/a | iter time: 326.05 ms\n",
      "Epoch 1 | iter 80 step 5 | loss train: 21.574, val: n/a | iter time: 248.46 ms (step)\n",
      "Epoch 1 | iter 90 step 5 | loss train: 21.169, val: n/a | iter time: 150.50 ms\n",
      "Epoch 1 | iter 100 step 6 | loss train: 21.287, val: n/a | iter time: 160.63 ms\n",
      "Epoch 1 | iter 110 step 6 | loss train: 20.559, val: n/a | iter time: 279.07 ms\n",
      "Epoch 1 | iter 120 step 7 | loss train: 21.118, val: n/a | iter time: 330.09 ms\n",
      "Epoch 1 | iter 130 step 8 | loss train: 21.205, val: n/a | iter time: 321.97 ms\n",
      "Epoch 1 | iter 140 step 8 | loss train: 20.628, val: n/a | iter time: 264.88 ms\n",
      "Epoch 1 | iter 150 step 9 | loss train: 20.355, val: n/a | iter time: 290.52 ms\n",
      "Epoch 1 | iter 160 step 10 | loss train: 20.289, val: n/a | iter time: 166.80 ms (step)\n",
      "Epoch 1 | iter 170 step 10 | loss train: 20.768, val: n/a | iter time: 317.82 ms\n",
      "Epoch 1 | iter 180 step 11 | loss train: 21.127, val: n/a | iter time: 234.57 ms\n",
      "Epoch 1 | iter 190 step 11 | loss train: 21.914, val: n/a | iter time: 316.71 ms\n",
      "Epoch 1 | iter 200 step 12 | loss train: 20.581, val: n/a | iter time: 320.58 ms\n",
      "Epoch 1 | iter 210 step 13 | loss train: 20.527, val: n/a | iter time: 282.67 ms\n",
      "Epoch 1 | iter 220 step 13 | loss train: 20.540, val: n/a | iter time: 200.03 ms\n",
      "Epoch 1 | iter 230 step 14 | loss train: 20.951, val: n/a | iter time: 418.89 ms\n",
      "Epoch 1 | iter 240 step 15 | loss train: 20.533, val: n/a | iter time: 161.09 ms (step)\n",
      "Epoch 1 | iter 250 step 15 | loss train: 20.813, val: n/a | iter time: 216.03 ms\n",
      "Epoch 1 | iter 260 step 16 | loss train: 21.377, val: n/a | iter time: 395.76 ms\n",
      "Epoch 1 | iter 270 step 16 | loss train: 20.802, val: n/a | iter time: 323.83 ms\n",
      "Epoch 1 | iter 280 step 17 | loss train: 21.146, val: n/a | iter time: 418.03 ms\n",
      "Epoch 1 | iter 290 step 18 | loss train: 21.070, val: n/a | iter time: 210.16 ms\n",
      "Epoch 1 | iter 300 step 18 | loss train: 20.505, val: n/a | iter time: 319.30 ms\n",
      "Epoch 1 | iter 310 step 19 | loss train: 19.518, val: n/a | iter time: 197.30 ms\n",
      "Epoch 1 | iter 320 step 20 | loss train: 19.756, val: n/a | iter time: 157.90 ms (step)\n",
      "Epoch 1 | iter 330 step 20 | loss train: 19.270, val: n/a | iter time: 227.74 ms\n",
      "Epoch 1 | iter 340 step 21 | loss train: 18.695, val: n/a | iter time: 164.17 ms\n",
      "Epoch 1 | iter 350 step 21 | loss train: 18.833, val: n/a | iter time: 199.13 ms\n",
      "Epoch 1 | iter 360 step 22 | loss train: 18.443, val: n/a | iter time: 145.11 ms\n",
      "Epoch 1 | iter 370 step 23 | loss train: 18.517, val: n/a | iter time: 149.90 ms\n",
      "Epoch 1 | iter 380 step 23 | loss train: 18.151, val: n/a | iter time: 287.49 ms\n",
      "Epoch 1 | iter 390 step 24 | loss train: 17.522, val: n/a | iter time: 146.67 ms\n",
      "Epoch 1 | iter 400 step 25 | loss train: 17.650, val: n/a | iter time: 211.99 ms (step)\n",
      "Epoch 1 | iter 410 step 25 | loss train: 17.964, val: n/a | iter time: 148.32 ms\n",
      "Epoch 1 | iter 420 step 26 | loss train: 17.372, val: n/a | iter time: 147.15 ms\n",
      "Epoch 1 | iter 430 step 26 | loss train: 16.749, val: n/a | iter time: 318.54 ms\n",
      "Epoch 1 | iter 440 step 27 | loss train: 16.613, val: n/a | iter time: 389.56 ms\n",
      "Epoch 1 | iter 450 step 28 | loss train: 16.238, val: n/a | iter time: 157.62 ms\n",
      "Epoch 1 | iter 460 step 28 | loss train: 15.948, val: n/a | iter time: 148.54 ms\n",
      "Epoch 1 | iter 470 step 29 | loss train: 15.786, val: n/a | iter time: 317.94 ms\n",
      "Epoch 1 | iter 480 step 30 | loss train: 15.284, val: n/a | iter time: 158.48 ms (step)\n",
      "Epoch 1 | iter 490 step 30 | loss train: 14.968, val: n/a | iter time: 324.56 ms\n",
      "Epoch 1 | iter 500 step 31 | loss train: 14.806, val: n/a | iter time: 419.01 ms\n",
      "Epoch 1 | iter 510 step 31 | loss train: 14.553, val: n/a | iter time: 236.09 ms\n",
      "Epoch 1 | iter 520 step 32 | loss train: 14.117, val: n/a | iter time: 136.59 ms\n",
      "Epoch 1 | iter 530 step 33 | loss train: 14.380, val: n/a | iter time: 288.44 ms\n",
      "Epoch 1 | iter 540 step 33 | loss train: 14.247, val: n/a | iter time: 244.09 ms\n",
      "Epoch 1 | iter 550 step 34 | loss train: 13.942, val: n/a | iter time: 200.11 ms\n",
      "Epoch 1 | iter 560 step 35 | loss train: 13.577, val: n/a | iter time: 329.51 ms (step)\n",
      "Epoch 1 | iter 570 step 35 | loss train: 13.447, val: n/a | iter time: 389.99 ms\n",
      "Epoch 1 | iter 580 step 36 | loss train: 13.310, val: n/a | iter time: 331.16 ms\n",
      "Epoch 1 | iter 590 step 36 | loss train: 12.816, val: n/a | iter time: 471.83 ms\n",
      "Epoch 1 | iter 600 step 37 | loss train: 12.599, val: n/a | iter time: 208.77 ms\n",
      "Epoch 1 | iter 610 step 38 | loss train: 12.627, val: n/a | iter time: 465.92 ms\n",
      "Epoch 1 | iter 620 step 38 | loss train: 12.233, val: n/a | iter time: 213.61 ms\n",
      "Epoch 1 | iter 630 step 39 | loss train: 12.012, val: n/a | iter time: 326.16 ms\n",
      "Epoch 1 | iter 640 step 40 | loss train: 11.918, val: n/a | iter time: 428.94 ms (step)\n",
      "Epoch 1 | iter 650 step 40 | loss train: 11.685, val: n/a | iter time: 285.87 ms\n",
      "Epoch 1 | iter 660 step 41 | loss train: 11.337, val: n/a | iter time: 198.14 ms\n",
      "Epoch 1 | iter 670 step 41 | loss train: 11.339, val: n/a | iter time: 282.36 ms\n",
      "Epoch 1 | iter 680 step 42 | loss train: 11.153, val: n/a | iter time: 324.38 ms\n",
      "Epoch 1 | iter 690 step 43 | loss train: 10.959, val: n/a | iter time: 212.79 ms\n",
      "Epoch 1 | iter 700 step 43 | loss train: 10.835, val: n/a | iter time: 401.13 ms\n",
      "Epoch 1 | iter 710 step 44 | loss train: 10.814, val: n/a | iter time: 329.80 ms\n",
      "Epoch 1 | iter 720 step 45 | loss train: 10.553, val: n/a | iter time: 245.40 ms (step)\n",
      "Epoch 1 | iter 730 step 45 | loss train: 10.469, val: n/a | iter time: 196.76 ms\n",
      "Epoch 1 | iter 740 step 46 | loss train: 10.468, val: n/a | iter time: 164.47 ms\n",
      "Epoch 1 | iter 750 step 46 | loss train: 10.444, val: n/a | iter time: 400.87 ms\n",
      "Epoch 1 | iter 760 step 47 | loss train: 10.235, val: n/a | iter time: 322.96 ms\n",
      "Epoch 1 | iter 770 step 48 | loss train: 10.301, val: n/a | iter time: 245.68 ms\n",
      "Epoch 1 | iter 780 step 48 | loss train: 10.221, val: n/a | iter time: 390.57 ms\n",
      "Epoch 1 | iter 790 step 49 | loss train: 10.193, val: n/a | iter time: 286.06 ms\n",
      "Epoch 1 | iter 800 step 50 | loss train: 9.893, val: n/a | iter time: 240.89 ms (step)\n",
      "Epoch 1 | iter 810 step 50 | loss train: 9.780, val: n/a | iter time: 289.38 ms\n",
      "Epoch 1 | iter 820 step 51 | loss train: 9.830, val: n/a | iter time: 318.92 ms\n",
      "Epoch 1 | iter 830 step 51 | loss train: 9.730, val: n/a | iter time: 158.89 ms\n",
      "Epoch 1 | iter 840 step 52 | loss train: 9.587, val: n/a | iter time: 198.74 ms\n",
      "Epoch 1 | iter 850 step 53 | loss train: 9.406, val: n/a | iter time: 279.39 ms\n",
      "Epoch 1 | iter 860 step 53 | loss train: 9.593, val: n/a | iter time: 389.81 ms\n",
      "Epoch 1 | iter 870 step 54 | loss train: 9.572, val: n/a | iter time: 479.62 ms\n",
      "Epoch 1 | iter 880 step 55 | loss train: 9.463, val: n/a | iter time: 429.40 ms (step)\n",
      "Epoch 1 | iter 890 step 55 | loss train: 9.465, val: n/a | iter time: 163.58 ms\n",
      "Epoch 1 | iter 900 step 56 | loss train: 9.329, val: n/a | iter time: 478.08 ms\n",
      "Epoch 1 | iter 910 step 56 | loss train: 9.345, val: n/a | iter time: 147.86 ms\n",
      "Epoch 1 | iter 920 step 57 | loss train: 9.212, val: n/a | iter time: 230.87 ms\n",
      "Epoch 1 | iter 930 step 58 | loss train: 9.141, val: n/a | iter time: 229.70 ms\n",
      "Epoch 1 | iter 940 step 58 | loss train: 8.786, val: n/a | iter time: 145.76 ms\n",
      "Epoch 1 | iter 950 step 59 | loss train: 8.868, val: n/a | iter time: 164.08 ms\n",
      "Epoch 2 | iter 960 step 60 | loss train: 8.862, val: n/a | iter time: 198.78 ms (step)\n",
      "Epoch 2 | iter 970 step 60 | loss train: 8.728, val: n/a | iter time: 152.45 ms\n",
      "Epoch 2 | iter 980 step 61 | loss train: 8.750, val: n/a | iter time: 161.70 ms\n",
      "Epoch 2 | iter 990 step 61 | loss train: 8.871, val: n/a | iter time: 283.74 ms\n",
      "Epoch 2 | iter 1000 step 62 | loss train: 8.728, val: n/a | iter time: 319.06 ms\n",
      "Epoch 2 | iter 1010 step 63 | loss train: 8.658, val: n/a | iter time: 240.75 ms\n",
      "Epoch 2 | iter 1020 step 63 | loss train: 8.612, val: n/a | iter time: 238.25 ms\n",
      "Epoch 2 | iter 1030 step 64 | loss train: 8.656, val: n/a | iter time: 420.95 ms\n",
      "Epoch 2 | iter 1040 step 65 | loss train: 8.525, val: n/a | iter time: 323.45 ms (step)\n",
      "Epoch 2 | iter 1050 step 65 | loss train: 8.477, val: n/a | iter time: 325.92 ms\n",
      "Epoch 2 | iter 1060 step 66 | loss train: 8.534, val: n/a | iter time: 278.12 ms\n",
      "Epoch 2 | iter 1070 step 66 | loss train: 8.366, val: n/a | iter time: 199.88 ms\n",
      "Epoch 2 | iter 1080 step 67 | loss train: 8.345, val: n/a | iter time: 390.91 ms\n",
      "Epoch 2 | iter 1090 step 68 | loss train: 8.165, val: n/a | iter time: 282.38 ms\n",
      "Epoch 2 | iter 1100 step 68 | loss train: 8.163, val: n/a | iter time: 159.94 ms\n",
      "Epoch 2 | iter 1110 step 69 | loss train: 8.307, val: n/a | iter time: 286.92 ms\n",
      "Epoch 2 | iter 1120 step 70 | loss train: 8.089, val: n/a | iter time: 162.56 ms (step)\n",
      "Epoch 2 | iter 1130 step 70 | loss train: 7.909, val: n/a | iter time: 277.31 ms\n",
      "Epoch 2 | iter 1140 step 71 | loss train: 8.114, val: n/a | iter time: 328.40 ms\n",
      "Epoch 2 | iter 1150 step 71 | loss train: 8.024, val: n/a | iter time: 325.15 ms\n",
      "Epoch 2 | iter 1160 step 72 | loss train: 7.848, val: n/a | iter time: 279.87 ms\n",
      "Epoch 2 | iter 1170 step 73 | loss train: 7.814, val: n/a | iter time: 321.34 ms\n",
      "Epoch 2 | iter 1180 step 73 | loss train: 7.862, val: n/a | iter time: 322.28 ms\n",
      "Epoch 2 | iter 1190 step 74 | loss train: 7.906, val: n/a | iter time: 293.20 ms\n",
      "Epoch 2 | iter 1200 step 75 | loss train: 8.042, val: n/a | iter time: 332.61 ms (step)\n",
      "Epoch 2 | iter 1210 step 75 | loss train: 7.885, val: n/a | iter time: 152.33 ms\n",
      "Epoch 2 | iter 1220 step 76 | loss train: 7.627, val: n/a | iter time: 321.33 ms\n",
      "Epoch 2 | iter 1230 step 76 | loss train: 7.659, val: n/a | iter time: 331.29 ms\n",
      "Epoch 2 | iter 1240 step 77 | loss train: 7.655, val: n/a | iter time: 210.75 ms\n",
      "Epoch 2 | iter 1250 step 78 | loss train: 7.513, val: n/a | iter time: 208.96 ms\n",
      "Epoch 2 | iter 1260 step 78 | loss train: 7.483, val: n/a | iter time: 279.09 ms\n",
      "Epoch 2 | iter 1270 step 79 | loss train: 7.480, val: n/a | iter time: 286.22 ms\n",
      "Epoch 2 | iter 1280 step 80 | loss train: 7.370, val: n/a | iter time: 282.53 ms (step)\n",
      "Epoch 2 | iter 1290 step 80 | loss train: 7.522, val: n/a | iter time: 160.73 ms\n",
      "Epoch 2 | iter 1300 step 81 | loss train: 7.046, val: n/a | iter time: 326.10 ms\n",
      "Epoch 2 | iter 1310 step 81 | loss train: 7.055, val: n/a | iter time: 328.78 ms\n",
      "Epoch 2 | iter 1320 step 82 | loss train: 7.245, val: n/a | iter time: 416.29 ms\n",
      "Epoch 2 | iter 1330 step 83 | loss train: 7.471, val: n/a | iter time: 210.26 ms\n",
      "Epoch 2 | iter 1340 step 83 | loss train: 7.436, val: n/a | iter time: 286.53 ms\n",
      "Epoch 2 | iter 1350 step 84 | loss train: 7.105, val: n/a | iter time: 280.72 ms\n",
      "Epoch 2 | iter 1360 step 85 | loss train: 6.972, val: n/a | iter time: 150.68 ms (step)\n",
      "Epoch 2 | iter 1370 step 85 | loss train: 7.069, val: n/a | iter time: 199.50 ms\n",
      "Epoch 2 | iter 1380 step 86 | loss train: 7.019, val: n/a | iter time: 195.07 ms\n",
      "Epoch 2 | iter 1390 step 86 | loss train: 6.792, val: n/a | iter time: 162.64 ms\n",
      "Epoch 2 | iter 1400 step 87 | loss train: 6.862, val: n/a | iter time: 242.16 ms\n",
      "Epoch 2 | iter 1410 step 88 | loss train: 6.542, val: n/a | iter time: 145.90 ms\n",
      "Epoch 2 | iter 1420 step 88 | loss train: 6.910, val: n/a | iter time: 230.42 ms\n",
      "Epoch 2 | iter 1430 step 89 | loss train: 6.553, val: n/a | iter time: 240.97 ms\n",
      "Epoch 2 | iter 1440 step 90 | loss train: 6.614, val: n/a | iter time: 277.22 ms (step)\n",
      "Epoch 2 | iter 1450 step 90 | loss train: 6.456, val: n/a | iter time: 161.55 ms\n",
      "Epoch 2 | iter 1460 step 91 | loss train: 6.717, val: n/a | iter time: 200.34 ms\n",
      "Epoch 2 | iter 1470 step 91 | loss train: 6.799, val: n/a | iter time: 211.31 ms\n",
      "Epoch 2 | iter 1480 step 92 | loss train: 6.488, val: n/a | iter time: 232.37 ms\n",
      "Epoch 2 | iter 1490 step 93 | loss train: 6.604, val: n/a | iter time: 228.46 ms\n",
      "Epoch 2 | iter 1500 step 93 | loss train: 6.544, val: n/a | iter time: 327.44 ms\n",
      "Epoch 2 | iter 1510 step 94 | loss train: 6.357, val: n/a | iter time: 211.58 ms\n",
      "Epoch 2 | iter 1520 step 95 | loss train: 6.346, val: n/a | iter time: 233.71 ms (step)\n",
      "Epoch 2 | iter 1530 step 95 | loss train: 6.218, val: n/a | iter time: 209.72 ms\n",
      "Epoch 2 | iter 1540 step 96 | loss train: 6.700, val: n/a | iter time: 286.15 ms\n",
      "Epoch 2 | iter 1550 step 96 | loss train: 6.550, val: n/a | iter time: 144.73 ms\n",
      "Epoch 2 | iter 1560 step 97 | loss train: 6.663, val: n/a | iter time: 161.13 ms\n",
      "Epoch 2 | iter 1570 step 98 | loss train: 6.257, val: n/a | iter time: 147.23 ms\n",
      "Epoch 2 | iter 1580 step 98 | loss train: 5.643, val: n/a | iter time: 210.72 ms\n",
      "Epoch 2 | iter 1590 step 99 | loss train: 6.182, val: n/a | iter time: 331.78 ms\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 | iter 1600 step 100 | loss train: 6.337, val: n/a | iter time: 167.02 ms (step)\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "\n",
      "### Response:\n",
      " الفريق.\n",
      ": have a the. digits and\n",
      " א4-'One text to4\n",
      "iter 1600: val loss 6.5178, val time: 6539.93 ms\n",
      "Epoch 2 | iter 1610 step 100 | loss train: 6.365, val: 6.518 | iter time: 395.15 ms\n",
      "Epoch 2 | iter 1620 step 101 | loss train: 6.477, val: 6.518 | iter time: 329.26 ms\n",
      "Epoch 2 | iter 1630 step 101 | loss train: 6.446, val: 6.518 | iter time: 317.53 ms\n",
      "Epoch 2 | iter 1640 step 102 | loss train: 6.413, val: 6.518 | iter time: 474.19 ms\n",
      "Epoch 2 | iter 1650 step 103 | loss train: 6.681, val: 6.518 | iter time: 210.29 ms\n",
      "Epoch 2 | iter 1660 step 103 | loss train: 6.662, val: 6.518 | iter time: 286.46 ms\n",
      "Epoch 2 | iter 1670 step 104 | loss train: 6.339, val: 6.518 | iter time: 198.90 ms\n",
      "Epoch 2 | iter 1680 step 105 | loss train: 6.072, val: 6.518 | iter time: 149.57 ms (step)\n",
      "Epoch 2 | iter 1690 step 105 | loss train: 5.715, val: 6.518 | iter time: 392.15 ms\n",
      "Epoch 2 | iter 1700 step 106 | loss train: 6.231, val: 6.518 | iter time: 237.56 ms\n",
      "Epoch 2 | iter 1710 step 106 | loss train: 6.046, val: 6.518 | iter time: 146.31 ms\n",
      "Epoch 2 | iter 1720 step 107 | loss train: 5.926, val: 6.518 | iter time: 243.21 ms\n",
      "Epoch 2 | iter 1730 step 108 | loss train: 5.641, val: 6.518 | iter time: 279.02 ms\n",
      "Epoch 2 | iter 1740 step 108 | loss train: 6.155, val: 6.518 | iter time: 210.83 ms\n",
      "Epoch 2 | iter 1750 step 109 | loss train: 6.306, val: 6.518 | iter time: 287.00 ms\n",
      "Epoch 2 | iter 1760 step 110 | loss train: 6.505, val: 6.518 | iter time: 321.58 ms (step)\n",
      "Epoch 2 | iter 1770 step 110 | loss train: 6.524, val: 6.518 | iter time: 239.79 ms\n",
      "Epoch 2 | iter 1780 step 111 | loss train: 6.343, val: 6.518 | iter time: 228.67 ms\n",
      "Epoch 2 | iter 1790 step 111 | loss train: 6.365, val: 6.518 | iter time: 157.85 ms\n",
      "Epoch 2 | iter 1800 step 112 | loss train: 6.092, val: 6.518 | iter time: 196.34 ms\n",
      "Epoch 2 | iter 1810 step 113 | loss train: 5.824, val: 6.518 | iter time: 326.85 ms\n",
      "Epoch 2 | iter 1820 step 113 | loss train: 6.160, val: 6.518 | iter time: 393.10 ms\n",
      "Epoch 2 | iter 1830 step 114 | loss train: 6.290, val: 6.518 | iter time: 284.56 ms\n",
      "Epoch 2 | iter 1840 step 115 | loss train: 6.158, val: 6.518 | iter time: 198.06 ms (step)\n",
      "Epoch 2 | iter 1850 step 115 | loss train: 6.061, val: 6.518 | iter time: 395.42 ms\n",
      "Epoch 2 | iter 1860 step 116 | loss train: 6.143, val: 6.518 | iter time: 331.20 ms\n",
      "Epoch 2 | iter 1870 step 116 | loss train: 5.906, val: 6.518 | iter time: 196.09 ms\n",
      "Epoch 2 | iter 1880 step 117 | loss train: 5.901, val: 6.518 | iter time: 466.12 ms\n",
      "Epoch 2 | iter 1890 step 118 | loss train: 6.167, val: 6.518 | iter time: 467.28 ms\n",
      "Epoch 2 | iter 1900 step 118 | loss train: 5.945, val: 6.518 | iter time: 283.40 ms\n",
      "Epoch 3 | iter 1910 step 119 | loss train: 5.874, val: 6.518 | iter time: 288.47 ms\n",
      "Epoch 3 | iter 1920 step 120 | loss train: 5.766, val: 6.518 | iter time: 148.75 ms (step)\n",
      "Epoch 3 | iter 1930 step 120 | loss train: 5.464, val: 6.518 | iter time: 161.19 ms\n",
      "Epoch 3 | iter 1940 step 121 | loss train: 5.643, val: 6.518 | iter time: 161.97 ms\n",
      "Epoch 3 | iter 1950 step 121 | loss train: 5.269, val: 6.518 | iter time: 465.83 ms\n",
      "Epoch 3 | iter 1960 step 122 | loss train: 5.880, val: 6.518 | iter time: 163.72 ms\n",
      "Epoch 3 | iter 1970 step 123 | loss train: 5.871, val: 6.518 | iter time: 198.16 ms\n",
      "Epoch 3 | iter 1980 step 123 | loss train: 6.016, val: 6.518 | iter time: 147.35 ms\n",
      "Epoch 3 | iter 1990 step 124 | loss train: 5.561, val: 6.518 | iter time: 322.45 ms\n",
      "Epoch 3 | iter 2000 step 125 | loss train: 5.695, val: 6.518 | iter time: 327.51 ms (step)\n",
      "Epoch 3 | iter 2010 step 125 | loss train: 6.135, val: 6.518 | iter time: 468.63 ms\n",
      "Epoch 3 | iter 2020 step 126 | loss train: 6.060, val: 6.518 | iter time: 391.46 ms\n",
      "Epoch 3 | iter 2030 step 126 | loss train: 5.603, val: 6.518 | iter time: 209.02 ms\n",
      "Epoch 3 | iter 2040 step 127 | loss train: 5.243, val: 6.518 | iter time: 413.58 ms\n",
      "Epoch 3 | iter 2050 step 128 | loss train: 5.371, val: 6.518 | iter time: 146.70 ms\n",
      "Epoch 3 | iter 2060 step 128 | loss train: 5.524, val: 6.518 | iter time: 231.86 ms\n",
      "Epoch 3 | iter 2070 step 129 | loss train: 5.849, val: 6.518 | iter time: 230.29 ms\n",
      "Epoch 3 | iter 2080 step 130 | loss train: 5.716, val: 6.518 | iter time: 321.03 ms (step)\n",
      "Epoch 3 | iter 2090 step 130 | loss train: 5.964, val: 6.518 | iter time: 391.92 ms\n",
      "Epoch 3 | iter 2100 step 131 | loss train: 5.562, val: 6.518 | iter time: 321.40 ms\n",
      "Epoch 3 | iter 2110 step 131 | loss train: 5.294, val: 6.518 | iter time: 153.02 ms\n",
      "Epoch 3 | iter 2120 step 132 | loss train: 5.744, val: 6.518 | iter time: 208.71 ms\n",
      "Epoch 3 | iter 2130 step 133 | loss train: 6.306, val: 6.518 | iter time: 397.42 ms\n",
      "Epoch 3 | iter 2140 step 133 | loss train: 6.253, val: 6.518 | iter time: 241.34 ms\n",
      "Epoch 3 | iter 2150 step 134 | loss train: 5.564, val: 6.518 | iter time: 146.68 ms\n",
      "Epoch 3 | iter 2160 step 135 | loss train: 5.844, val: 6.518 | iter time: 159.66 ms (step)\n",
      "Epoch 3 | iter 2170 step 135 | loss train: 5.643, val: 6.518 | iter time: 159.55 ms\n",
      "Epoch 3 | iter 2180 step 136 | loss train: 5.188, val: 6.518 | iter time: 212.56 ms\n",
      "Epoch 3 | iter 2190 step 136 | loss train: 6.028, val: 6.518 | iter time: 389.21 ms\n",
      "Epoch 3 | iter 2200 step 137 | loss train: 6.065, val: 6.518 | iter time: 163.39 ms\n",
      "Epoch 3 | iter 2210 step 138 | loss train: 5.778, val: 6.518 | iter time: 147.08 ms\n",
      "Epoch 3 | iter 2220 step 138 | loss train: 5.882, val: 6.518 | iter time: 389.57 ms\n",
      "Epoch 3 | iter 2230 step 139 | loss train: 6.110, val: 6.518 | iter time: 228.18 ms\n",
      "Epoch 3 | iter 2240 step 140 | loss train: 5.841, val: 6.518 | iter time: 325.10 ms (step)\n",
      "Epoch 3 | iter 2250 step 140 | loss train: 5.490, val: 6.518 | iter time: 160.37 ms\n",
      "Epoch 3 | iter 2260 step 141 | loss train: 5.408, val: 6.518 | iter time: 229.82 ms\n",
      "Epoch 3 | iter 2270 step 141 | loss train: 5.812, val: 6.518 | iter time: 397.21 ms\n",
      "Epoch 3 | iter 2280 step 142 | loss train: 6.375, val: 6.518 | iter time: 332.16 ms\n",
      "Epoch 3 | iter 2290 step 143 | loss train: 5.889, val: 6.518 | iter time: 210.49 ms\n",
      "Epoch 3 | iter 2300 step 143 | loss train: 5.922, val: 6.518 | iter time: 147.35 ms\n",
      "Epoch 3 | iter 2310 step 144 | loss train: 5.345, val: 6.518 | iter time: 329.81 ms\n",
      "Epoch 3 | iter 2320 step 145 | loss train: 6.176, val: 6.518 | iter time: 280.35 ms (step)\n",
      "Epoch 3 | iter 2330 step 145 | loss train: 5.722, val: 6.518 | iter time: 148.41 ms\n",
      "Epoch 3 | iter 2340 step 146 | loss train: 5.390, val: 6.518 | iter time: 283.35 ms\n",
      "Epoch 3 | iter 2350 step 146 | loss train: 5.956, val: 6.518 | iter time: 283.20 ms\n",
      "Epoch 3 | iter 2360 step 147 | loss train: 5.631, val: 6.518 | iter time: 243.46 ms\n",
      "Epoch 3 | iter 2370 step 148 | loss train: 5.712, val: 6.518 | iter time: 287.33 ms\n",
      "Epoch 3 | iter 2380 step 148 | loss train: 5.682, val: 6.518 | iter time: 285.81 ms\n",
      "Epoch 3 | iter 2390 step 149 | loss train: 5.607, val: 6.518 | iter time: 317.24 ms\n",
      "Epoch 3 | iter 2400 step 150 | loss train: 5.624, val: 6.518 | iter time: 215.55 ms (step)\n",
      "Epoch 3 | iter 2410 step 150 | loss train: 5.767, val: 6.518 | iter time: 153.92 ms\n",
      "Epoch 3 | iter 2420 step 151 | loss train: 5.884, val: 6.518 | iter time: 197.28 ms\n",
      "Epoch 3 | iter 2430 step 151 | loss train: 5.456, val: 6.518 | iter time: 148.89 ms\n",
      "Epoch 3 | iter 2440 step 152 | loss train: 5.327, val: 6.518 | iter time: 318.75 ms\n",
      "Epoch 3 | iter 2450 step 153 | loss train: 5.482, val: 6.518 | iter time: 331.48 ms\n",
      "Epoch 3 | iter 2460 step 153 | loss train: 5.948, val: 6.518 | iter time: 331.91 ms\n",
      "Epoch 3 | iter 2470 step 154 | loss train: 5.953, val: 6.518 | iter time: 233.74 ms\n",
      "Epoch 3 | iter 2480 step 155 | loss train: 6.104, val: 6.518 | iter time: 278.91 ms (step)\n",
      "Epoch 3 | iter 2490 step 155 | loss train: 5.853, val: 6.518 | iter time: 145.92 ms\n",
      "Epoch 3 | iter 2500 step 156 | loss train: 5.824, val: 6.518 | iter time: 284.54 ms\n",
      "Epoch 3 | iter 2510 step 156 | loss train: 5.472, val: 6.518 | iter time: 331.59 ms\n",
      "Epoch 3 | iter 2520 step 157 | loss train: 5.838, val: 6.518 | iter time: 331.13 ms\n",
      "Epoch 3 | iter 2530 step 158 | loss train: 6.064, val: 6.518 | iter time: 328.18 ms\n",
      "Epoch 3 | iter 2540 step 158 | loss train: 5.242, val: 6.518 | iter time: 230.26 ms\n",
      "Epoch 3 | iter 2550 step 159 | loss train: 5.173, val: 6.518 | iter time: 285.33 ms\n",
      "Epoch 3 | iter 2560 step 160 | loss train: 5.357, val: 6.518 | iter time: 200.99 ms (step)\n",
      "Epoch 3 | iter 2570 step 160 | loss train: 5.870, val: 6.518 | iter time: 285.97 ms\n",
      "Epoch 3 | iter 2580 step 161 | loss train: 5.742, val: 6.518 | iter time: 209.88 ms\n",
      "Epoch 3 | iter 2590 step 161 | loss train: 5.645, val: 6.518 | iter time: 323.26 ms\n",
      "Epoch 3 | iter 2600 step 162 | loss train: 6.245, val: 6.518 | iter time: 282.38 ms\n",
      "Epoch 3 | iter 2610 step 163 | loss train: 5.664, val: 6.518 | iter time: 197.44 ms\n",
      "Epoch 3 | iter 2620 step 163 | loss train: 5.648, val: 6.518 | iter time: 208.32 ms\n",
      "Epoch 3 | iter 2630 step 164 | loss train: 5.691, val: 6.518 | iter time: 209.36 ms\n",
      "Epoch 3 | iter 2640 step 165 | loss train: 5.838, val: 6.518 | iter time: 200.25 ms (step)\n",
      "Epoch 3 | iter 2650 step 165 | loss train: 5.997, val: 6.518 | iter time: 330.54 ms\n",
      "Epoch 3 | iter 2660 step 166 | loss train: 5.859, val: 6.518 | iter time: 321.12 ms\n",
      "Epoch 3 | iter 2670 step 166 | loss train: 5.945, val: 6.518 | iter time: 144.83 ms\n",
      "Epoch 3 | iter 2680 step 167 | loss train: 5.586, val: 6.518 | iter time: 197.67 ms\n",
      "Epoch 3 | iter 2690 step 168 | loss train: 5.325, val: 6.518 | iter time: 398.22 ms\n",
      "Epoch 3 | iter 2700 step 168 | loss train: 5.517, val: 6.518 | iter time: 394.36 ms\n",
      "Epoch 3 | iter 2710 step 169 | loss train: 5.519, val: 6.518 | iter time: 286.11 ms\n",
      "Epoch 3 | iter 2720 step 170 | loss train: 5.336, val: 6.518 | iter time: 214.56 ms (step)\n",
      "Epoch 3 | iter 2730 step 170 | loss train: 5.560, val: 6.518 | iter time: 416.17 ms\n",
      "Epoch 3 | iter 2740 step 171 | loss train: 5.289, val: 6.518 | iter time: 208.06 ms\n",
      "Epoch 3 | iter 2750 step 171 | loss train: 5.532, val: 6.518 | iter time: 199.26 ms\n",
      "Epoch 3 | iter 2760 step 172 | loss train: 5.953, val: 6.518 | iter time: 234.65 ms\n",
      "Epoch 3 | iter 2770 step 173 | loss train: 5.685, val: 6.518 | iter time: 430.02 ms\n",
      "Epoch 3 | iter 2780 step 173 | loss train: 5.747, val: 6.518 | iter time: 210.03 ms\n",
      "Epoch 3 | iter 2790 step 174 | loss train: 5.208, val: 6.518 | iter time: 200.78 ms\n",
      "Epoch 3 | iter 2800 step 175 | loss train: 5.100, val: 6.518 | iter time: 396.85 ms (step)\n",
      "Epoch 3 | iter 2810 step 175 | loss train: 5.655, val: 6.518 | iter time: 289.26 ms\n",
      "Epoch 3 | iter 2820 step 176 | loss train: 6.245, val: 6.518 | iter time: 419.54 ms\n",
      "Epoch 3 | iter 2830 step 176 | loss train: 5.645, val: 6.518 | iter time: 197.03 ms\n",
      "Epoch 3 | iter 2840 step 177 | loss train: 5.058, val: 6.518 | iter time: 276.33 ms\n",
      "Epoch 3 | iter 2850 step 178 | loss train: 5.295, val: 6.518 | iter time: 230.49 ms\n",
      "Training time: 766.71s\n",
      "Memory used: 11.08 GB\n",
      "Saving LoRA weights to 'out/google/gemma-2b/final/lit_model.pth.lora'\n",
      "Saved merged weights to 'out/google/gemma-2b/final/lit_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# 2) Finetune the model\n",
    "!litgpt finetune lora \\\n",
    "  --checkpoint_dir checkpoints/google/gemma-2b \\\n",
    "  --precision bf16-true \\\n",
    "  --quantize bnb.fp4 \\\n",
    "  --train.micro_batch_size 2 \\\n",
    "  --train.log_interval 10 \\\n",
    "  --train.save_interval 250 \\\n",
    "  --train.global_batch_size 32 \\\n",
    "  --train.lr_warmup_steps=100 \\\n",
    "  --train.epochs 3 \\\n",
    "  --data Alpaca2k \\\n",
    "  --out_dir out/google/gemma-2b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
