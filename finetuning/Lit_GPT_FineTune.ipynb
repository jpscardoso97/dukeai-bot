{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torchaudio torchdata torchtext torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzguU9H0GLgk",
        "outputId": "49f0a317-352c-4322-e6c4-aa5f5649f684"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchaudio 2.2.1+cu121\n",
            "Uninstalling torchaudio-2.2.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.2.1+cu121\n",
            "Found existing installation: torchdata 0.7.1\n",
            "Uninstalling torchdata-0.7.1:\n",
            "  Successfully uninstalled torchdata-0.7.1\n",
            "Found existing installation: torchtext 0.17.1\n",
            "Uninstalling torchtext-0.17.1:\n",
            "  Successfully uninstalled torchtext-0.17.1\n",
            "Found existing installation: torchvision 0.17.1+cu121\n",
            "Uninstalling torchvision-0.17.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2_5yzCx_AMP",
        "outputId": "bd794f92-faab-46e4-c2c8-39d17dee9a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.4/136.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.2/192.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m928.4/928.4 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/viztracer/\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install 'litgpt[all]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List available models\n",
        "!litgpt download"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u7EhZvX_DLu",
        "outputId": "229b203a-2909-48ca-e456-c6703cc3f73d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please specify --repo_id <repo_id>. Available values:\n",
            "stabilityai/stablelm-base-alpha-3b\n",
            "stabilityai/stablelm-base-alpha-7b\n",
            "stabilityai/stablelm-tuned-alpha-3b\n",
            "stabilityai/stablelm-tuned-alpha-7b\n",
            "stabilityai/stablelm-3b-4e1t\n",
            "stabilityai/stablelm-zephyr-3b\n",
            "stabilityai/stablecode-completion-alpha-3b\n",
            "stabilityai/stablecode-completion-alpha-3b-4k\n",
            "stabilityai/stablecode-instruct-alpha-3b\n",
            "stabilityai/stable-code-3b\n",
            "EleutherAI/pythia-14m\n",
            "EleutherAI/pythia-31m\n",
            "EleutherAI/pythia-70m\n",
            "EleutherAI/pythia-160m\n",
            "EleutherAI/pythia-410m\n",
            "EleutherAI/pythia-1b\n",
            "EleutherAI/pythia-1.4b\n",
            "EleutherAI/pythia-2.8b\n",
            "EleutherAI/pythia-6.9b\n",
            "EleutherAI/pythia-12b\n",
            "EleutherAI/pythia-70m-deduped\n",
            "EleutherAI/pythia-160m-deduped\n",
            "EleutherAI/pythia-410m-deduped\n",
            "EleutherAI/pythia-1b-deduped\n",
            "EleutherAI/pythia-1.4b-deduped\n",
            "EleutherAI/pythia-2.8b-deduped\n",
            "EleutherAI/pythia-6.9b-deduped\n",
            "EleutherAI/pythia-12b-deduped\n",
            "databricks/dolly-v2-3b\n",
            "databricks/dolly-v2-7b\n",
            "databricks/dolly-v2-12b\n",
            "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
            "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
            "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
            "togethercomputer/RedPajama-INCITE-7B-Base\n",
            "togethercomputer/RedPajama-INCITE-7B-Chat\n",
            "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
            "togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n",
            "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n",
            "togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n",
            "tiiuae/falcon-7b\n",
            "tiiuae/falcon-7b-instruct\n",
            "tiiuae/falcon-40b\n",
            "tiiuae/falcon-40b-instruct\n",
            "tiiuae/falcon-180B\n",
            "tiiuae/falcon-180B-chat\n",
            "openlm-research/open_llama_3b\n",
            "openlm-research/open_llama_7b\n",
            "openlm-research/open_llama_13b\n",
            "lmsys/vicuna-7b-v1.3\n",
            "lmsys/vicuna-13b-v1.3\n",
            "lmsys/vicuna-33b-v1.3\n",
            "lmsys/vicuna-7b-v1.5\n",
            "lmsys/vicuna-7b-v1.5-16k\n",
            "lmsys/vicuna-13b-v1.5\n",
            "lmsys/vicuna-13b-v1.5-16k\n",
            "lmsys/longchat-7b-16k\n",
            "lmsys/longchat-13b-16k\n",
            "NousResearch/Nous-Hermes-llama-2-7b\n",
            "NousResearch/Nous-Hermes-13b\n",
            "NousResearch/Nous-Hermes-Llama2-13b\n",
            "meta-llama/Llama-2-7b-hf\n",
            "meta-llama/Llama-2-7b-chat-hf\n",
            "meta-llama/Llama-2-13b-hf\n",
            "meta-llama/Llama-2-13b-chat-hf\n",
            "meta-llama/Llama-2-70b-hf\n",
            "meta-llama/Llama-2-70b-chat-hf\n",
            "google/gemma-2b\n",
            "google/gemma-7b\n",
            "google/gemma-2b-it\n",
            "google/gemma-7b-it\n",
            "stabilityai/FreeWilly2\n",
            "codellama/CodeLlama-7b-hf\n",
            "codellama/CodeLlama-13b-hf\n",
            "codellama/CodeLlama-34b-hf\n",
            "codellama/CodeLlama-70b-hf\n",
            "codellama/CodeLlama-7b-Python-hf\n",
            "codellama/CodeLlama-13b-Python-hf\n",
            "codellama/CodeLlama-34b-Python-hf\n",
            "codellama/CodeLlama-70b-Python-hf\n",
            "codellama/CodeLlama-7b-Instruct-hf\n",
            "codellama/CodeLlama-13b-Instruct-hf\n",
            "codellama/CodeLlama-34b-Instruct-hf\n",
            "codellama/CodeLlama-70b-Instruct-hf\n",
            "garage-bAInd/Platypus-30B\n",
            "garage-bAInd/Platypus2-7B\n",
            "garage-bAInd/Platypus2-13B\n",
            "garage-bAInd/Platypus2-70B\n",
            "garage-bAInd/Camel-Platypus2-13B\n",
            "garage-bAInd/Camel-Platypus2-70B\n",
            "garage-bAInd/Stable-Platypus2-13B\n",
            "garage-bAInd/Platypus2-70B-instruct\n",
            "togethercomputer/LLaMA-2-7B-32K\n",
            "microsoft/phi-1_5\n",
            "microsoft/phi-2\n",
            "mistralai/Mistral-7B-v0.1\n",
            "mistralai/Mistral-7B-Instruct-v0.1\n",
            "mistralai/Mixtral-8x7B-v0.1\n",
            "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
            "mistralai/Mistral-7B-Instruct-v0.2\n",
            "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Download a pretrained model\n",
        "!litgpt download --repo_id microsoft/phi-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMjtaiIw_FZ9",
        "outputId": "05acd736-e248-41b8-f705-a327eccb82ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "config.json: 100% 863/863 [00:00<00:00, 4.83MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 739kB/s]\n",
            "model-00001-of-00002.safetensors: 100% 5.00G/5.00G [00:46<00:00, 107MB/s] \n",
            "model-00002-of-00002.safetensors: 100% 564M/564M [00:05<00:00, 99.3MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 16.6MB/s]\n",
            "tokenizer_config.json: 100% 7.34k/7.34k [00:00<00:00, 28.2MB/s]\n",
            "Converting .safetensor files to PyTorch binaries (.bin)\n",
            "checkpoints/microsoft/phi-2/model-00001-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00001-of-00002.bin\n",
            "checkpoints/microsoft/phi-2/model-00002-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00002-of-00002.bin\n",
            "Converting checkpoint files to LitGPT format.\n",
            "Processing checkpoints/microsoft/phi-2/model-00001-of-00002.bin\n",
            "Loading 'model.embed_tokens.weight' into RAM\n",
            "Loading 'model.layers.0.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.0.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.0.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.0.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.0.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.0.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.1.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.1.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.1.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.1.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.1.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.1.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.10.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.10.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.10.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.10.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.10.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.10.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.10.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.10.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.11.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.11.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.11.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.11.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.11.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.11.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.12.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.12.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.12.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.12.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.12.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.12.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.13.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.13.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.13.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.13.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.13.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.13.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.14.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.14.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.14.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.14.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.14.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.14.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.15.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.15.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.15.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.15.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.15.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.15.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.16.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.16.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.16.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.16.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.16.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.16.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.17.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.17.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.17.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.17.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.17.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.17.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.18.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.18.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.18.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.18.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.18.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.18.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.19.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.19.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.19.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.19.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.19.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.19.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.2.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.2.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.2.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.2.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.2.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.2.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.20.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.20.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.20.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.20.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.20.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.20.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.21.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.21.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.21.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.21.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.21.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.21.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.22.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.22.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.22.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.22.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.22.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.22.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.22.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.22.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.23.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.23.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.23.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.23.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.23.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.23.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.24.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.24.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.24.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.24.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.24.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.24.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.25.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.25.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.25.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.25.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.25.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.25.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.26.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.26.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.26.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.26.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.26.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.26.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.27.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.27.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.27.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.27.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.27.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.27.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.28.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.28.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.28.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.28.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.28.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.28.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.29.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.29.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.29.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.29.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.29.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.29.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.3.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.3.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.3.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.3.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.3.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.3.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.4.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.4.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.4.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.4.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.4.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.4.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.5.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.5.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.5.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.5.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.5.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.5.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.6.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.6.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.6.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.6.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.6.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.6.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.7.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.7.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.7.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.7.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.7.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.7.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.8.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.8.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.8.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.8.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.8.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.8.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.9.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.9.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.9.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.9.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.9.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.9.self_attn.dense.weight' into RAM\n",
            "Loading 'layer 0 q bias' into RAM\n",
            "Loading 'layer 0 k bias' into RAM\n",
            "Loading 'layer 0 v bias' into RAM\n",
            "Loading 'layer 0 q weight' into RAM\n",
            "Loading 'layer 0 k weight' into RAM\n",
            "Loading 'layer 0 v weight' into RAM\n",
            "Loading 'layer 1 q bias' into RAM\n",
            "Loading 'layer 1 k bias' into RAM\n",
            "Loading 'layer 1 v bias' into RAM\n",
            "Loading 'layer 1 q weight' into RAM\n",
            "Loading 'layer 1 k weight' into RAM\n",
            "Loading 'layer 1 v weight' into RAM\n",
            "Loading 'layer 10 q bias' into RAM\n",
            "Loading 'layer 10 k bias' into RAM\n",
            "Loading 'layer 10 v bias' into RAM\n",
            "Loading 'layer 10 q weight' into RAM\n",
            "Loading 'layer 10 k weight' into RAM\n",
            "Loading 'layer 10 v weight' into RAM\n",
            "Loading 'layer 11 q bias' into RAM\n",
            "Loading 'layer 11 k bias' into RAM\n",
            "Loading 'layer 11 v bias' into RAM\n",
            "Loading 'layer 11 q weight' into RAM\n",
            "Loading 'layer 11 k weight' into RAM\n",
            "Loading 'layer 11 v weight' into RAM\n",
            "Loading 'layer 12 q bias' into RAM\n",
            "Loading 'layer 12 k bias' into RAM\n",
            "Loading 'layer 12 v bias' into RAM\n",
            "Loading 'layer 12 q weight' into RAM\n",
            "Loading 'layer 12 k weight' into RAM\n",
            "Loading 'layer 12 v weight' into RAM\n",
            "Loading 'layer 13 q bias' into RAM\n",
            "Loading 'layer 13 k bias' into RAM\n",
            "Loading 'layer 13 v bias' into RAM\n",
            "Loading 'layer 13 q weight' into RAM\n",
            "Loading 'layer 13 k weight' into RAM\n",
            "Loading 'layer 13 v weight' into RAM\n",
            "Loading 'layer 14 q bias' into RAM\n",
            "Loading 'layer 14 k bias' into RAM\n",
            "Loading 'layer 14 v bias' into RAM\n",
            "Loading 'layer 14 q weight' into RAM\n",
            "Loading 'layer 14 k weight' into RAM\n",
            "Loading 'layer 14 v weight' into RAM\n",
            "Loading 'layer 15 q bias' into RAM\n",
            "Loading 'layer 15 k bias' into RAM\n",
            "Loading 'layer 15 v bias' into RAM\n",
            "Loading 'layer 15 q weight' into RAM\n",
            "Loading 'layer 15 k weight' into RAM\n",
            "Loading 'layer 15 v weight' into RAM\n",
            "Loading 'layer 16 q bias' into RAM\n",
            "Loading 'layer 16 k bias' into RAM\n",
            "Loading 'layer 16 v bias' into RAM\n",
            "Loading 'layer 16 q weight' into RAM\n",
            "Loading 'layer 16 k weight' into RAM\n",
            "Loading 'layer 16 v weight' into RAM\n",
            "Loading 'layer 17 q bias' into RAM\n",
            "Loading 'layer 17 k bias' into RAM\n",
            "Loading 'layer 17 v bias' into RAM\n",
            "Loading 'layer 17 q weight' into RAM\n",
            "Loading 'layer 17 k weight' into RAM\n",
            "Loading 'layer 17 v weight' into RAM\n",
            "Loading 'layer 18 q bias' into RAM\n",
            "Loading 'layer 18 k bias' into RAM\n",
            "Loading 'layer 18 v bias' into RAM\n",
            "Loading 'layer 18 q weight' into RAM\n",
            "Loading 'layer 18 k weight' into RAM\n",
            "Loading 'layer 18 v weight' into RAM\n",
            "Loading 'layer 19 q bias' into RAM\n",
            "Loading 'layer 19 k bias' into RAM\n",
            "Loading 'layer 19 v bias' into RAM\n",
            "Loading 'layer 19 q weight' into RAM\n",
            "Loading 'layer 19 k weight' into RAM\n",
            "Loading 'layer 19 v weight' into RAM\n",
            "Loading 'layer 2 q bias' into RAM\n",
            "Loading 'layer 2 k bias' into RAM\n",
            "Loading 'layer 2 v bias' into RAM\n",
            "Loading 'layer 2 q weight' into RAM\n",
            "Loading 'layer 2 k weight' into RAM\n",
            "Loading 'layer 2 v weight' into RAM\n",
            "Loading 'layer 20 q bias' into RAM\n",
            "Loading 'layer 20 k bias' into RAM\n",
            "Loading 'layer 20 v bias' into RAM\n",
            "Loading 'layer 20 q weight' into RAM\n",
            "Loading 'layer 20 k weight' into RAM\n",
            "Loading 'layer 20 v weight' into RAM\n",
            "Loading 'layer 21 q bias' into RAM\n",
            "Loading 'layer 21 k bias' into RAM\n",
            "Loading 'layer 21 v bias' into RAM\n",
            "Loading 'layer 21 q weight' into RAM\n",
            "Loading 'layer 21 k weight' into RAM\n",
            "Loading 'layer 21 v weight' into RAM\n",
            "Loading 'layer 22 q bias' into RAM\n",
            "Loading 'layer 22 k bias' into RAM\n",
            "Loading 'layer 22 v bias' into RAM\n",
            "Loading 'layer 22 q weight' into RAM\n",
            "Loading 'layer 22 k weight' into RAM\n",
            "Loading 'layer 22 v weight' into RAM\n",
            "Loading 'layer 23 q bias' into RAM\n",
            "Loading 'layer 23 k bias' into RAM\n",
            "Loading 'layer 23 v bias' into RAM\n",
            "Loading 'layer 23 q weight' into RAM\n",
            "Loading 'layer 23 k weight' into RAM\n",
            "Loading 'layer 23 v weight' into RAM\n",
            "Loading 'layer 24 q bias' into RAM\n",
            "Loading 'layer 24 k bias' into RAM\n",
            "Loading 'layer 24 v bias' into RAM\n",
            "Loading 'layer 24 q weight' into RAM\n",
            "Loading 'layer 24 k weight' into RAM\n",
            "Loading 'layer 24 v weight' into RAM\n",
            "Loading 'layer 25 q bias' into RAM\n",
            "Loading 'layer 25 k bias' into RAM\n",
            "Loading 'layer 25 v bias' into RAM\n",
            "Loading 'layer 25 q weight' into RAM\n",
            "Loading 'layer 25 k weight' into RAM\n",
            "Loading 'layer 25 v weight' into RAM\n",
            "Loading 'layer 26 q bias' into RAM\n",
            "Loading 'layer 26 k bias' into RAM\n",
            "Loading 'layer 26 v bias' into RAM\n",
            "Loading 'layer 26 q weight' into RAM\n",
            "Loading 'layer 26 k weight' into RAM\n",
            "Loading 'layer 26 v weight' into RAM\n",
            "Loading 'layer 27 q bias' into RAM\n",
            "Loading 'layer 27 k bias' into RAM\n",
            "Loading 'layer 27 v bias' into RAM\n",
            "Loading 'layer 27 q weight' into RAM\n",
            "Loading 'layer 27 k weight' into RAM\n",
            "Loading 'layer 27 v weight' into RAM\n",
            "Loading 'layer 28 q bias' into RAM\n",
            "Loading 'layer 28 k bias' into RAM\n",
            "Loading 'layer 28 v bias' into RAM\n",
            "Loading 'layer 28 q weight' into RAM\n",
            "Loading 'layer 28 k weight' into RAM\n",
            "Loading 'layer 28 v weight' into RAM\n",
            "Loading 'layer 29 q bias' into RAM\n",
            "Loading 'layer 29 k bias' into RAM\n",
            "Loading 'layer 29 v bias' into RAM\n",
            "Loading 'layer 29 q weight' into RAM\n",
            "Loading 'layer 29 k weight' into RAM\n",
            "Loading 'layer 29 v weight' into RAM\n",
            "Loading 'layer 3 q bias' into RAM\n",
            "Loading 'layer 3 k bias' into RAM\n",
            "Loading 'layer 3 v bias' into RAM\n",
            "Loading 'layer 3 q weight' into RAM\n",
            "Loading 'layer 3 k weight' into RAM\n",
            "Loading 'layer 3 v weight' into RAM\n",
            "Loading 'layer 4 q bias' into RAM\n",
            "Loading 'layer 4 k bias' into RAM\n",
            "Loading 'layer 4 v bias' into RAM\n",
            "Loading 'layer 4 q weight' into RAM\n",
            "Loading 'layer 4 k weight' into RAM\n",
            "Loading 'layer 4 v weight' into RAM\n",
            "Loading 'layer 5 q bias' into RAM\n",
            "Loading 'layer 5 k bias' into RAM\n",
            "Loading 'layer 5 v bias' into RAM\n",
            "Loading 'layer 5 q weight' into RAM\n",
            "Loading 'layer 5 k weight' into RAM\n",
            "Loading 'layer 5 v weight' into RAM\n",
            "Loading 'layer 6 q bias' into RAM\n",
            "Loading 'layer 6 k bias' into RAM\n",
            "Loading 'layer 6 v bias' into RAM\n",
            "Loading 'layer 6 q weight' into RAM\n",
            "Loading 'layer 6 k weight' into RAM\n",
            "Loading 'layer 6 v weight' into RAM\n",
            "Loading 'layer 7 q bias' into RAM\n",
            "Loading 'layer 7 k bias' into RAM\n",
            "Loading 'layer 7 v bias' into RAM\n",
            "Loading 'layer 7 q weight' into RAM\n",
            "Loading 'layer 7 k weight' into RAM\n",
            "Loading 'layer 7 v weight' into RAM\n",
            "Loading 'layer 8 q bias' into RAM\n",
            "Loading 'layer 8 k bias' into RAM\n",
            "Loading 'layer 8 v bias' into RAM\n",
            "Loading 'layer 8 q weight' into RAM\n",
            "Loading 'layer 8 k weight' into RAM\n",
            "Loading 'layer 8 v weight' into RAM\n",
            "Loading 'layer 9 q bias' into RAM\n",
            "Loading 'layer 9 k bias' into RAM\n",
            "Loading 'layer 9 v bias' into RAM\n",
            "Loading 'layer 9 q weight' into RAM\n",
            "Loading 'layer 9 k weight' into RAM\n",
            "Loading 'layer 9 v weight' into RAM\n",
            "Processing checkpoints/microsoft/phi-2/model-00002-of-00002.bin\n",
            "Loading 'lm_head.bias' into RAM\n",
            "Loading 'lm_head.weight' into RAM\n",
            "Loading 'model.final_layernorm.bias' into RAM\n",
            "Loading 'model.final_layernorm.weight' into RAM\n",
            "Loading 'model.layers.30.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.30.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.30.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.30.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.30.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.30.self_attn.dense.weight' into RAM\n",
            "Loading 'model.layers.31.input_layernorm.bias' into RAM\n",
            "Loading 'model.layers.31.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.fc1.bias' into RAM\n",
            "Loading 'model.layers.31.mlp.fc1.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.fc2.bias' into RAM\n",
            "Loading 'model.layers.31.mlp.fc2.weight' into RAM\n",
            "Loading 'model.layers.31.self_attn.dense.bias' into RAM\n",
            "Loading 'model.layers.31.self_attn.dense.weight' into RAM\n",
            "Loading 'layer 30 q bias' into RAM\n",
            "Loading 'layer 30 k bias' into RAM\n",
            "Loading 'layer 30 v bias' into RAM\n",
            "Loading 'layer 30 q weight' into RAM\n",
            "Loading 'layer 30 k weight' into RAM\n",
            "Loading 'layer 30 v weight' into RAM\n",
            "Loading 'layer 31 q bias' into RAM\n",
            "Loading 'layer 31 k bias' into RAM\n",
            "Loading 'layer 31 v bias' into RAM\n",
            "Loading 'layer 31 q weight' into RAM\n",
            "Loading 'layer 31 k weight' into RAM\n",
            "Loading 'layer 31 v weight' into RAM\n",
            "Saving converted checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2) Chat with the pretrained model\n",
        "!litgpt chat \\\n",
        "  --checkpoint_dir checkpoints/microsoft/phi-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSN_OycsNThn",
        "outputId": "3985007c-4f4c-4256-bfff-ef3db32fb0db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now chatting with phi-2.\n",
            "To exit, press 'Enter' on an empty prompt.\n",
            "\n",
            "Seed set to 1234\n",
            ">> Prompt: Holy Fawn is a\n",
            ">> Reply:  A book cover of 'Holy Fawn' could feature a serene and tranquil landscape. It could showcase a peaceful meadow with tall grasses swaying in the breeze, a gently flowing stream with water reflecting the sunlight, and a majestic deer grazing in the distance. The title of the book should be in a soft and elegant font, with a touch of gold or silver to convey its sacredness and spirituality.\n",
            "\n",
            "Time for inference: 4.23 sec total, 19.85 tokens/sec, 84 tokens\n",
            "\n",
            ">> Prompt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Finetune the model\n",
        "!litgpt finetune lora \\\n",
        "  --checkpoint_dir checkpoints/microsoft/phi-2 \\\n",
        "  --quantize bnb.fp4 \\\n",
        "  --train.micro_batch_size 4 \\\n",
        "  --train.learning_rate 0.0003 \\\n",
        "  --train.weight_decay 0.01 \\\n",
        "  --train.global_batch_size 16 \\\n",
        "  --lora_r 8 \\\n",
        "  --lora_alpha 16 \\\n",
        "  --lora_dropout 0.05 \\\n",
        "  --train.lr_warmup_steps 100 \\\n",
        "  --data Alpaca2k \\\n",
        "  --out_dir out/microsoft/phi-2-lora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW9xYQgLHfb3",
        "outputId": "0e853bd5-9310-4cfa-98dd-7c71e20ea183"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'checkpoint_dir': PosixPath('checkpoints/microsoft/phi-2'),\n",
            " 'data': Alpaca2k(mask_prompt=False,\n",
            "                  val_split_fraction=0.05,\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7f5f9e5f2140>,\n",
            "                  ignore_index=-100,\n",
            "                  seed=42,\n",
            "                  num_workers=4,\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\n",
            " 'devices': 1,\n",
            " 'eval': EvalArgs(interval=100, max_new_tokens=100, max_iters=100),\n",
            " 'logger_name': 'csv',\n",
            " 'lora_alpha': 16,\n",
            " 'lora_dropout': 0.05,\n",
            " 'lora_head': False,\n",
            " 'lora_key': False,\n",
            " 'lora_mlp': False,\n",
            " 'lora_projection': False,\n",
            " 'lora_query': True,\n",
            " 'lora_r': 8,\n",
            " 'lora_value': True,\n",
            " 'out_dir': PosixPath('out/microsoft/phi-2-lora'),\n",
            " 'precision': None,\n",
            " 'quantize': 'bnb.fp4',\n",
            " 'seed': 1337,\n",
            " 'train': TrainArgs(save_interval=1000,\n",
            "                    log_interval=1,\n",
            "                    global_batch_size=16,\n",
            "                    micro_batch_size=4,\n",
            "                    lr_warmup_steps=100,\n",
            "                    epochs=5,\n",
            "                    max_tokens=None,\n",
            "                    max_steps=None,\n",
            "                    max_seq_length=None,\n",
            "                    tie_embeddings=None,\n",
            "                    learning_rate=0.0003,\n",
            "                    weight_decay=0.01,\n",
            "                    beta1=0.9,\n",
            "                    beta2=0.95,\n",
            "                    max_norm=None,\n",
            "                    min_lr=6e-05)}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/litgpt\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litgpt/__main__.py\", line 129, in main\n",
            "    fn(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litgpt/finetune/lora.py\", line 116, in setup\n",
            "    raise ValueError(\"Quantization and mixed precision is not supported.\")\n",
            "ValueError: Quantization and mixed precision is not supported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Chat with the model\n",
        "!litgpt chat \\\n",
        "  --checkpoint_dir out/phi-2-lora/final"
      ],
      "metadata": {
        "id": "jBevLqYzHovP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}